{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepCovid_squeezenetNewDataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw2wjP5rHk2v",
        "colab_type": "code",
        "outputId": "1b330dd6-fd0b-4329-f404-c208198ae7b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQgXM59JR2LW",
        "colab_type": "code",
        "outputId": "910cca94-ac08-4005-bc4e-780667ed55e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#use https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "# Top level data directory. Here we assume the format of the directory conforms\n",
        "#   to the ImageFolder structure\n",
        "data_dir = \"./drive/My Drive/data_covid5k_augmented/\"\n",
        "\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"squeezenet\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 20\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 100\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch Version:  1.5.0+cu101\n",
            "Torchvision Version:  0.6.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w83THTSrR6qF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCA_oFKAtXLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'test' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'test':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OALK_gt6tcXE",
        "colab_type": "code",
        "outputId": "0f745a26-b74e-497a-891b-7cb0491a704d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"inception\":\n",
        "        \"\"\" Inception v3\n",
        "        Be careful, expects (299,299) sized images and has auxiliary output\n",
        "        \"\"\"\n",
        "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        # Handle the auxilary net\n",
        "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
        "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        # Handle the primary net\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 299\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SqueezeNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (3): Fire(\n",
            "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Fire(\n",
            "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Fire(\n",
            "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (7): Fire(\n",
            "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (8): Fire(\n",
            "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (9): Fire(\n",
            "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (10): Fire(\n",
            "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "    (12): Fire(\n",
            "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (squeeze_activation): ReLU(inplace=True)\n",
            "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (expand1x1_activation): ReLU(inplace=True)\n",
            "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (expand3x3_activation): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCWoz70ex5jc",
        "colab_type": "code",
        "outputId": "7c131580-35fa-456c-94fa-46f0726f4a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "# Create training and validation datasets\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'test']}\n",
        "# Create training and validation dataloaders\n",
        "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'test']}\n",
        "\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing Datasets and Dataloaders...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P3iwJyLyKLH",
        "colab_type": "code",
        "outputId": "0ad28658-169c-4729-b608-5d77bd261e9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Send the model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Gather the parameters to be optimized/updated in this run. If we are\n",
        "#  finetuning we will be updating all parameters. However, if we are\n",
        "#  doing feature extract method, we will only update the parameters\n",
        "#  that we have just initialized, i.e. the parameters with requires_grad\n",
        "#  is True.\n",
        "params_to_update = model_ft.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model_ft.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t classifier.1.weight\n",
            "\t classifier.1.bias\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFd8LadGyOO-",
        "colab_type": "code",
        "outputId": "f4b17653-1535-4a85-96de-3a44acc0afe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Setup the loss fxn\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Train and evaluate\n",
        "model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 0.5563 Acc: 0.1212\n",
            "test Loss: 0.5823 Acc: 0.9855\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 0.5307 Acc: 0.1250\n",
            "test Loss: 0.5483 Acc: 0.9868\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 0.5312 Acc: 0.1250\n",
            "test Loss: 0.5946 Acc: 0.9855\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 0.5285 Acc: 0.1250\n",
            "test Loss: 0.6421 Acc: 0.9727\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 0.5281 Acc: 0.1250\n",
            "test Loss: 0.6182 Acc: 0.9799\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 0.5320 Acc: 0.1250\n",
            "test Loss: 0.6278 Acc: 0.9737\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 0.5335 Acc: 0.1250\n",
            "test Loss: 0.6377 Acc: 0.9635\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 0.5290 Acc: 0.1250\n",
            "test Loss: 0.6549 Acc: 0.9658\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 0.5340 Acc: 0.1250\n",
            "test Loss: 0.6170 Acc: 0.9809\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 0.5317 Acc: 0.1250\n",
            "test Loss: 0.6397 Acc: 0.9750\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 0.5279 Acc: 0.1250\n",
            "test Loss: 0.5573 Acc: 0.9862\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 0.5283 Acc: 0.1250\n",
            "test Loss: 0.6218 Acc: 0.9803\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 0.5284 Acc: 0.1250\n",
            "test Loss: 0.6245 Acc: 0.9832\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 0.5297 Acc: 0.1250\n",
            "test Loss: 0.6405 Acc: 0.9780\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 0.5324 Acc: 0.1250\n",
            "test Loss: 0.6275 Acc: 0.9829\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 0.5276 Acc: 0.1250\n",
            "test Loss: 0.6511 Acc: 0.9592\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 0.5282 Acc: 0.1250\n",
            "test Loss: 0.6287 Acc: 0.9806\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 0.5252 Acc: 0.1250\n",
            "test Loss: 0.6305 Acc: 0.9766\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 0.5240 Acc: 0.1250\n",
            "test Loss: 0.6419 Acc: 0.9707\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 0.5299 Acc: 0.1250\n",
            "test Loss: 0.6351 Acc: 0.9780\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 0.5283 Acc: 0.1250\n",
            "test Loss: 0.6462 Acc: 0.9740\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 0.5283 Acc: 0.1250\n",
            "test Loss: 0.6265 Acc: 0.9816\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 0.5250 Acc: 0.1250\n",
            "test Loss: 0.6525 Acc: 0.9197\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 0.5281 Acc: 0.1250\n",
            "test Loss: 0.6528 Acc: 0.9368\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 0.5232 Acc: 0.1250\n",
            "test Loss: 0.6392 Acc: 0.9720\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.5301 Acc: 0.1250\n",
            "test Loss: 0.6143 Acc: 0.9799\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.5239 Acc: 0.1250\n",
            "test Loss: 0.6292 Acc: 0.9793\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.5314 Acc: 0.1250\n",
            "test Loss: 0.6265 Acc: 0.9753\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.5278 Acc: 0.1250\n",
            "test Loss: 0.6372 Acc: 0.9658\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.5266 Acc: 0.1250\n",
            "test Loss: 0.6295 Acc: 0.9743\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 0.5265 Acc: 0.1250\n",
            "test Loss: 0.6492 Acc: 0.9612\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.5261 Acc: 0.1250\n",
            "test Loss: 0.6319 Acc: 0.9793\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.5330 Acc: 0.1250\n",
            "test Loss: 0.6533 Acc: 0.9599\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.5280 Acc: 0.1250\n",
            "test Loss: 0.6369 Acc: 0.9701\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.5255 Acc: 0.1250\n",
            "test Loss: 0.6274 Acc: 0.9714\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.5254 Acc: 0.1250\n",
            "test Loss: 0.6259 Acc: 0.9809\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.5304 Acc: 0.1250\n",
            "test Loss: 0.6213 Acc: 0.9770\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.5288 Acc: 0.1250\n",
            "test Loss: 0.6307 Acc: 0.9743\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.5303 Acc: 0.1250\n",
            "test Loss: 0.6353 Acc: 0.9688\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.5299 Acc: 0.1250\n",
            "test Loss: 0.6318 Acc: 0.9681\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.5282 Acc: 0.1250\n",
            "test Loss: 0.6496 Acc: 0.9582\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.5279 Acc: 0.1250\n",
            "test Loss: 0.6385 Acc: 0.9743\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.5251 Acc: 0.1250\n",
            "test Loss: 0.6207 Acc: 0.9796\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.5261 Acc: 0.1250\n",
            "test Loss: 0.6508 Acc: 0.9326\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.5277 Acc: 0.1250\n",
            "test Loss: 0.6455 Acc: 0.9661\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.5236 Acc: 0.1250\n",
            "test Loss: 0.6176 Acc: 0.9747\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.5289 Acc: 0.1250\n",
            "test Loss: 0.6290 Acc: 0.9497\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.5256 Acc: 0.1250\n",
            "test Loss: 0.6028 Acc: 0.9730\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.5253 Acc: 0.1250\n",
            "test Loss: 0.6185 Acc: 0.9671\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.5222 Acc: 0.1250\n",
            "test Loss: 0.6319 Acc: 0.9536\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.5309 Acc: 0.1250\n",
            "test Loss: 0.6538 Acc: 0.9375\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.5250 Acc: 0.1250\n",
            "test Loss: 0.6202 Acc: 0.9786\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.5313 Acc: 0.1250\n",
            "test Loss: 0.6304 Acc: 0.9740\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.5223 Acc: 0.1250\n",
            "test Loss: 0.6509 Acc: 0.9441\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.5268 Acc: 0.1250\n",
            "test Loss: 0.6597 Acc: 0.8984\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 0.5245 Acc: 0.1250\n",
            "test Loss: 0.6502 Acc: 0.9398\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 0.5273 Acc: 0.1250\n",
            "test Loss: 0.6373 Acc: 0.9530\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 0.5280 Acc: 0.1250\n",
            "test Loss: 0.6520 Acc: 0.9286\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 0.5293 Acc: 0.1250\n",
            "test Loss: 0.6347 Acc: 0.9688\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 0.5300 Acc: 0.1250\n",
            "test Loss: 0.6536 Acc: 0.9470\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 0.5327 Acc: 0.1250\n",
            "test Loss: 0.6405 Acc: 0.9655\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 0.5307 Acc: 0.1250\n",
            "test Loss: 0.6539 Acc: 0.9382\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 0.5246 Acc: 0.1250\n",
            "test Loss: 0.6380 Acc: 0.9586\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 0.5292 Acc: 0.1250\n",
            "test Loss: 0.6426 Acc: 0.9513\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 0.5311 Acc: 0.1250\n",
            "test Loss: 0.6455 Acc: 0.9572\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 0.5263 Acc: 0.1250\n",
            "test Loss: 0.6441 Acc: 0.9520\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 0.5304 Acc: 0.1250\n",
            "test Loss: 0.6388 Acc: 0.9645\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 0.5320 Acc: 0.1250\n",
            "test Loss: 0.6587 Acc: 0.9342\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 0.5251 Acc: 0.1250\n",
            "test Loss: 0.6678 Acc: 0.8701\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 0.5298 Acc: 0.1250\n",
            "test Loss: 0.6496 Acc: 0.9493\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 0.5290 Acc: 0.1250\n",
            "test Loss: 0.6632 Acc: 0.9092\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 0.5323 Acc: 0.1250\n",
            "test Loss: 0.6689 Acc: 0.8743\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 0.5270 Acc: 0.1250\n",
            "test Loss: 0.6545 Acc: 0.9595\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 0.5278 Acc: 0.1250\n",
            "test Loss: 0.6641 Acc: 0.9079\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 0.5250 Acc: 0.1250\n",
            "test Loss: 0.6563 Acc: 0.8918\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 0.5312 Acc: 0.1250\n",
            "test Loss: 0.6467 Acc: 0.9441\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 0.5267 Acc: 0.1250\n",
            "test Loss: 0.6380 Acc: 0.9582\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 0.5073 Acc: 0.1250\n",
            "test Loss: 0.6526 Acc: 0.9247\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 0.5301 Acc: 0.1250\n",
            "test Loss: 0.6547 Acc: 0.9197\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 0.5307 Acc: 0.1250\n",
            "test Loss: 0.6406 Acc: 0.9533\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 0.5258 Acc: 0.1250\n",
            "test Loss: 0.6429 Acc: 0.9299\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 0.5296 Acc: 0.1250\n",
            "test Loss: 0.6300 Acc: 0.9730\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n",
            "train Loss: 0.5292 Acc: 0.1250\n",
            "test Loss: 0.6702 Acc: 0.8806\n",
            "\n",
            "Epoch 83/99\n",
            "----------\n",
            "train Loss: 0.5372 Acc: 0.1250\n",
            "test Loss: 0.6612 Acc: 0.9194\n",
            "\n",
            "Epoch 84/99\n",
            "----------\n",
            "train Loss: 0.5364 Acc: 0.1250\n",
            "test Loss: 0.6626 Acc: 0.9145\n",
            "\n",
            "Epoch 85/99\n",
            "----------\n",
            "train Loss: 0.5301 Acc: 0.1250\n",
            "test Loss: 0.6663 Acc: 0.8750\n",
            "\n",
            "Epoch 86/99\n",
            "----------\n",
            "train Loss: 0.5377 Acc: 0.1250\n",
            "test Loss: 0.6540 Acc: 0.9250\n",
            "\n",
            "Epoch 87/99\n",
            "----------\n",
            "train Loss: 0.5342 Acc: 0.1250\n",
            "test Loss: 0.6515 Acc: 0.9385\n",
            "\n",
            "Epoch 88/99\n",
            "----------\n",
            "train Loss: 0.5380 Acc: 0.1250\n",
            "test Loss: 0.6683 Acc: 0.8612\n",
            "\n",
            "Epoch 89/99\n",
            "----------\n",
            "train Loss: 0.5352 Acc: 0.1250\n",
            "test Loss: 0.6693 Acc: 0.8582\n",
            "\n",
            "Epoch 90/99\n",
            "----------\n",
            "train Loss: 0.5363 Acc: 0.1250\n",
            "test Loss: 0.6414 Acc: 0.9671\n",
            "\n",
            "Epoch 91/99\n",
            "----------\n",
            "train Loss: 0.5384 Acc: 0.1250\n",
            "test Loss: 0.6594 Acc: 0.9237\n",
            "\n",
            "Epoch 92/99\n",
            "----------\n",
            "train Loss: 0.5420 Acc: 0.1250\n",
            "test Loss: 0.6458 Acc: 0.9539\n",
            "\n",
            "Epoch 93/99\n",
            "----------\n",
            "train Loss: 0.5371 Acc: 0.1250\n",
            "test Loss: 0.6561 Acc: 0.9339\n",
            "\n",
            "Epoch 94/99\n",
            "----------\n",
            "train Loss: 0.5415 Acc: 0.1250\n",
            "test Loss: 0.6645 Acc: 0.8990\n",
            "\n",
            "Epoch 95/99\n",
            "----------\n",
            "train Loss: 0.5406 Acc: 0.1250\n",
            "test Loss: 0.6425 Acc: 0.9437\n",
            "\n",
            "Epoch 96/99\n",
            "----------\n",
            "train Loss: 0.5346 Acc: 0.1250\n",
            "test Loss: 0.6584 Acc: 0.8572\n",
            "\n",
            "Epoch 97/99\n",
            "----------\n",
            "train Loss: 0.5392 Acc: 0.1250\n",
            "test Loss: 0.6536 Acc: 0.9132\n",
            "\n",
            "Epoch 98/99\n",
            "----------\n",
            "train Loss: 0.5343 Acc: 0.1250\n",
            "test Loss: 0.6574 Acc: 0.9115\n",
            "\n",
            "Epoch 99/99\n",
            "----------\n",
            "train Loss: 0.5360 Acc: 0.1250\n",
            "test Loss: 0.6670 Acc: 0.8569\n",
            "\n",
            "Training complete in 176m 17s\n",
            "Best val Acc: 0.986842\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr0OoO09Y-1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model_ft, './drive/My Drive/covid_squeezenet_epoch%d_newdataset.pt'%100)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}